
import os
import sys
import logging
import time
from pathlib import Path
from tqdm import tqdm

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from rag.graph_rag import GraphRAG
from data.supabase_client import SupabaseClient

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def build_relationships(batch_size=50, limit=1000, offset=0):
    """
    documents í…Œì´ë¸”ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜
        limit: ì „ì²´ ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜ ì œí•œ
        offset: ì‹œì‘ ìœ„ì¹˜ (í˜ì´ì§€ë„¤ì´ì…˜)
    """
    logger.info(f"ğŸš€ ê¸°ì—… ê´€ê³„ êµ¬ì¶• ì‹œì‘... (Offset: {offset}, Limit: {limit})")
    
    try:
        graph_rag = GraphRAG()
        supabase = graph_rag.supabase
        
        # 0. ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œ ID ê°€ì ¸ì˜¤ê¸°
        logger.info("ğŸ” ê¸°ì²˜ë¦¬ ë¬¸ì„œ í™•ì¸ ì¤‘...")
        processed_docs = set()
        try:
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ extracted_from ì²´í¬ëŠ” ë¡œì»¬ ë©”ëª¨ë¦¬ë³´ë‹¨ ê±´ë„ˆë›°ê¸° ì „ëµì´ ë‚«ì§€ë§Œ
            # ì¼ë‹¨ ì•ˆì „ì„ ìœ„í•´ ì²´í¬í•©ë‹ˆë‹¤. 
            # (ì£¼ì˜: ë³‘ë ¬ ì‹¤í–‰ ì‹œ processed_docsê°€ ì‹¤ì‹œê°„ ë™ê¸°í™”ë˜ì§„ ì•Šì§€ë§Œ, ì¤‘ë³µ ì €ì¥ì€ í° ë¬¸ì œ ì—†ìŠµë‹ˆë‹¤)
            rels = supabase.table("company_relationships").select("extracted_from").execute()
            for r in rels.data:
                if r.get("extracted_from"):
                    processed_docs.add(r["extracted_from"])
            logger.info(f"âœ… ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œ: {len(processed_docs)}ê°œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ê¸°ì²˜ë¦¬ ë¬¸ì„œ í™•ì¸ ì‹¤íŒ¨: {e}")

        # 1. ì²˜ë¦¬í•  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        logger.info("ğŸ“š ë¬¸ì„œ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        # rangeë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›
        query = supabase.table("documents").select("id, content, metadata").range(offset, offset + limit - 1)
        
        result = query.execute()
        documents = result.data
        
        if not documents:
            logger.warning("âŒ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        logger.info(f"âœ… {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. (ID: {documents[0]['id'][:8]}... ~ {documents[-1]['id'][:8]}...)")
        
        total_extracted = 0
        skipped_count = 0
        
        # 2. ë¬¸ì„œë³„ ê´€ê³„ ì¶”ì¶œ
        for i, doc in enumerate(tqdm(documents, desc="Processing Documents")):
            doc_id = doc.get("id")
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œë©´ ê±´ë„ˆë›°ê¸°
            if str(doc_id) in processed_docs:
                skipped_count += 1
                continue
                
            content = doc.get("content", "")
            metadata = doc.get("metadata") or {}
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ í‹°ì»¤ ì •ë³´ê°€ ìˆìœ¼ë©´ íŒíŠ¸ë¡œ í™œìš©
            source_ticker = None
            if isinstance(metadata, dict):
                source_ticker = metadata.get("ticker")
            
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
            if len(content) < 100:
                continue
                
            # ê´€ê³„ ì¶”ì¶œ (LLM í˜¸ì¶œ)
            # ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ 2000ìë§Œ ì‚¬ìš©
            relationships = graph_rag.extract_relationships(content[:2000], source_ticker=source_ticker)
            
            if relationships:
                # ì €ì¥
                saved_count = graph_rag.save_relationships(
                    relationships, 
                    extracted_from=str(doc_id),
                    filing_date=doc.get("metadata", {}).get("date")
                )
                total_extracted += saved_count
                
            # Rate Limit ë°©ì§€
            time.sleep(0.5)
            
            if (i + 1) % batch_size == 0:
                logger.info(f"ğŸ”„ ì¤‘ê°„ ì§‘ê³„: {i+1}/{len(documents)} ì²˜ë¦¬, {total_extracted}ê°œ ê´€ê³„ ì €ì¥")

        logger.info("="*50)
        logger.info(f"ğŸ‰ ì™„ë£Œ! ì´ {total_extracted}ê°œì˜ ìƒˆë¡œìš´ ê¸°ì—… ê´€ê³„ê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤. (Skipped: {skipped_count})")
        logger.info("="*50)

    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    # ì‹¤í–‰ ì‹œ ì¸ìë¡œ limit, offset ì¡°ì ˆ ê°€ëŠ¥
    # usage: python script.py [limit] [offset]
    limit = 1000
    offset = 0
    
    if len(sys.argv) > 1:
        try:
            limit = int(sys.argv[1])
        except:
            pass
            
    if len(sys.argv) > 2:
        try:
            offset = int(sys.argv[2])
        except:
            pass
            
    print(f"ğŸ”§ ì„¤ì •: ë¬¸ì„œ {limit}ê°œ ì²˜ë¦¬ (Offset: {offset})")
    build_relationships(limit=limit, offset=offset)
