"""
SEC EDGAR 10-K ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ë° ê´€ê³„ ì •ë³´ ì¶”ì¶œ

ìˆ˜ì§‘ëœ 105ê°œ ê¸°ì—…ì˜ 10-K ì „ì²´ ë¬¸ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ ,
ê¸°ì—… ê°„ ê´€ê³„ ì •ë³´(ê³µê¸‰ì—…ì²´, ê³ ê°, ê²½ìŸì‚¬, ìíšŒì‚¬)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""
import os
import re
import json
import time
import requests
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from bs4 import BeautifulSoup
import pandas as pd
from dotenv import load_dotenv

load_dotenv()

# SEC API ì„¤ì •
SEC_BASE_URL = "https://www.sec.gov"
SEC_EDGAR_URL = "https://data.sec.gov"

# 105ê°œ ê¸°ì—… í‹°ì»¤ ëª©ë¡
TOP_COMPANIES = [
    "AAPL", "MSFT", "GOOGL", "AMZN", "NVDA", "META", "TSLA", "BRK-B", "TSM", "AVGO",
    "JPM", "V", "MA", "BAC", "WFC", "GS", "MS", "AXP", "SPGI", "BLK",
    "UNH", "JNJ", "LLY", "PFE", "ABBV", "MRK", "TMO", "ABT", "DHR", "BMY",
    "WMT", "PG", "KO", "PEP", "COST", "MCD", "NKE", "DIS", "SBUX", "TGT",
    "CAT", "GE", "HON", "UNP", "RTX", "BA", "LMT", "DE", "UPS", "MMM",
    "XOM", "CVX", "COP", "SLB", "EOG", "MPC", "PSX", "VLO", "OXY", "KMI",
    "VZ", "T", "TMUS", "CMCSA", "CHTR",
    "PLD", "AMT", "EQIX", "CCI", "SPG",
    "NEE", "DUK", "SO", "D", "AEP",
    "ORCL", "CRM", "ADBE", "AMD", "INTC", "QCOM", "TXN", "AMAT", "MU", "LRCX",
    "ASML", "NOW", "INTU", "PYPL", "ISRG", "BKNG", "MDLZ", "ADP", "CI", "REGN",
    "CVS", "GILD", "VRTX", "AMGN", "ZTS", "SYK", "BDX", "ELV", "HUM", "MCK"
]

# ê´€ê³„ í‚¤ì›Œë“œ íŒ¨í„´
RELATIONSHIP_PATTERNS = {
    "supplier": [
        r"(?:our |the )?(?:primary |major |key |principal )?supplier[s]?(?:\s+include|\s+are|\s+such as)?[\s:]+([A-Z][A-Za-z\s&,]+)",
        r"(?:we |the company )?(?:source[s]? |purchase[s]? |procure[s]? )(?:from|through)\s+([A-Z][A-Za-z\s&,]+)",
        r"(?:manufactured |produced |supplied )(?:by|from)\s+([A-Z][A-Za-z\s&,]+)",
    ],
    "customer": [
        r"(?:our |the )?(?:largest |major |key |principal )?customer[s]?(?:\s+include|\s+are)?[\s:]+([A-Z][A-Za-z\s&,]+)",
        r"(?:we |the company )?(?:sell[s]? |provide[s]? )(?:to|services? to)\s+([A-Z][A-Za-z\s&,]+)",
        r"revenue[s]? from\s+([A-Z][A-Za-z\s&,]+)",
    ],
    "competitor": [
        r"(?:our |the )?(?:primary |major |key )?competitor[s]?(?:\s+include|\s+are)?[\s:]+([A-Z][A-Za-z\s&,]+)",
        r"(?:we )?compete[s]? (?:with|against)\s+([A-Z][A-Za-z\s&,]+)",
        r"competition from\s+([A-Z][A-Za-z\s&,]+)",
    ],
    "subsidiary": [
        r"(?:our )?(?:wholly[- ]owned )?subsidiar(?:y|ies)(?:\s+include)?[\s:]+([A-Z][A-Za-z\s&,]+)",
        r"(?:we )?(?:own[s]?|acquired)\s+([A-Z][A-Za-z\s&,]+)",
    ],
    "partner": [
        r"(?:our |the )?(?:strategic )?partner(?:ship)?[s]?(?:\s+with|\s+include)?[\s:]+([A-Z][A-Za-z\s&,]+)",
        r"(?:joint venture|collaboration|alliance)\s+(?:with|between)\s+([A-Z][A-Za-z\s&,]+)",
    ],
}

# ì•Œë ¤ì§„ ê¸°ì—…ëª… ëª©ë¡ (ë§¤ì¹­ ì •í™•ë„ í–¥ìƒìš©)
KNOWN_COMPANIES = set([
    "Apple", "Microsoft", "Google", "Alphabet", "Amazon", "Meta", "Facebook",
    "NVIDIA", "Tesla", "TSMC", "Taiwan Semiconductor", "Broadcom", "Qualcomm",
    "Intel", "AMD", "Samsung", "SK Hynix", "Micron", "Texas Instruments",
    "JPMorgan", "Goldman Sachs", "Morgan Stanley", "Bank of America", "Wells Fargo",
    "Visa", "Mastercard", "American Express", "PayPal",
    "Johnson & Johnson", "Pfizer", "Merck", "AbbVie", "Bristol-Myers",
    "UnitedHealth", "CVS", "Cigna", "Anthem", "Humana",
    "Walmart", "Target", "Costco", "Amazon", "Home Depot",
    "Coca-Cola", "PepsiCo", "McDonald's", "Starbucks", "Nike", "Disney",
    "ExxonMobil", "Chevron", "ConocoPhillips", "Shell", "BP",
    "AT&T", "Verizon", "T-Mobile", "Comcast", "Charter",
    "Boeing", "Lockheed Martin", "Raytheon", "General Dynamics", "Northrop Grumman",
    "Caterpillar", "Deere", "3M", "Honeywell", "General Electric",
    "Oracle", "Salesforce", "Adobe", "SAP", "ServiceNow", "Intuit",
    "Foxconn", "Hon Hai", "Pegatron", "Wistron", "Luxshare",
])


def get_user_agent():
    """SEC API ìš”ì²­ì— í•„ìš”í•œ User-Agent"""
    email = os.getenv("SEC_API_USER_AGENT", "researcher@university.edu")
    return f"Mozilla/5.0 (compatible; ResearchBot/1.0; +mailto:{email})"


def get_company_cik_map() -> Dict[str, dict]:
    """í‹°ì»¤-CIK ë§¤í•‘ ì¡°íšŒ"""
    headers = {
        "User-Agent": get_user_agent(),
        "Accept": "application/json"
    }
    
    url = "https://www.sec.gov/files/company_tickers.json"
    response = requests.get(url, headers=headers, timeout=30)
    response.raise_for_status()
    data = response.json()
    
    cik_map = {}
    for item in data.values():
        ticker = item.get("ticker", "").upper()
        cik = str(item.get("cik_str", "")).zfill(10)
        title = item.get("title", "")
        cik_map[ticker] = {"cik": cik, "title": title}
    
    return cik_map


def get_10k_filing_url(cik: str, headers: dict) -> Optional[Tuple[str, str]]:
    """ê°€ì¥ ìµœê·¼ 10-K íŒŒì¼ë§ URL ì¡°íšŒ"""
    submissions_url = f"{SEC_EDGAR_URL}/submissions/CIK{cik}.json"
    
    try:
        response = requests.get(submissions_url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        recent_filings = data.get("filings", {}).get("recent", {})
        forms = recent_filings.get("form", [])
        accessions = recent_filings.get("accessionNumber", [])
        primary_docs = recent_filings.get("primaryDocument", [])
        filing_dates = recent_filings.get("filingDate", [])
        
        for i, form in enumerate(forms):
            if form == "10-K":
                accession = accessions[i].replace("-", "")
                primary_doc = primary_docs[i]
                filing_date = filing_dates[i]
                
                doc_url = f"{SEC_BASE_URL}/Archives/edgar/data/{cik.lstrip('0')}/{accession}/{primary_doc}"
                return doc_url, filing_date
        
        return None, None
    
    except Exception as e:
        print(f"      10-K ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None, None


def download_10k_document(url: str, headers: dict) -> Optional[str]:
    """10-K ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ"""
    try:
        response = requests.get(url, headers=headers, timeout=60)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"      ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None


def extract_text_from_html(html_content: str) -> str:
    """HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    soup = BeautifulSoup(html_content, "html.parser")
    
    # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
    for tag in soup(["script", "style", "meta", "link"]):
        tag.decompose()
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = soup.get_text(separator=" ", strip=True)
    
    # ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    
    return text


def extract_sections(text: str) -> Dict[str, str]:
    """10-K ì£¼ìš” ì„¹ì…˜ ì¶”ì¶œ"""
    sections = {}
    
    # Item 1 - Business
    item1_pattern = r"(?:ITEM\s*1\.?\s*[-â€“â€”]?\s*BUSINESS)(.*?)(?:ITEM\s*1A|ITEM\s*2)"
    match = re.search(item1_pattern, text, re.IGNORECASE | re.DOTALL)
    if match:
        sections["business"] = match.group(1)[:50000]  # ìµœëŒ€ 50K ë¬¸ì
    
    # Item 1A - Risk Factors
    item1a_pattern = r"(?:ITEM\s*1A\.?\s*[-â€“â€”]?\s*RISK\s*FACTORS)(.*?)(?:ITEM\s*1B|ITEM\s*2)"
    match = re.search(item1a_pattern, text, re.IGNORECASE | re.DOTALL)
    if match:
        sections["risk_factors"] = match.group(1)[:50000]
    
    # Item 7 - MD&A
    item7_pattern = r"(?:ITEM\s*7\.?\s*[-â€“â€”]?\s*MANAGEMENT)(.*?)(?:ITEM\s*7A|ITEM\s*8)"
    match = re.search(item7_pattern, text, re.IGNORECASE | re.DOTALL)
    if match:
        sections["mda"] = match.group(1)[:50000]
    
    return sections


def extract_relationships(text: str, source_company: str) -> List[Dict]:
    """í…ìŠ¤íŠ¸ì—ì„œ ê¸°ì—… ê´€ê³„ ì¶”ì¶œ"""
    relationships = []
    
    for rel_type, patterns in RELATIONSHIP_PATTERNS.items():
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            
            for match in matches:
                # ë§¤ì¹˜ëœ ê¸°ì—…ëª… ì •ë¦¬
                companies = clean_company_names(match)
                
                for company in companies:
                    if company and len(company) > 2:
                        relationships.append({
                            "source": source_company,
                            "target": company,
                            "type": rel_type,
                        })
    
    # ì•Œë ¤ì§„ ê¸°ì—…ëª… ì§ì ‘ ê²€ìƒ‰
    for known_company in KNOWN_COMPANIES:
        if known_company.lower() in text.lower() and known_company != source_company:
            # ì´ë¯¸ ì¶”ê°€ëœ ê´€ê³„ì¸ì§€ í™•ì¸
            existing = any(r["target"].lower() == known_company.lower() for r in relationships)
            if not existing:
                relationships.append({
                    "source": source_company,
                    "target": known_company,
                    "type": "mentioned",
                })
    
    return relationships


def clean_company_names(text: str) -> List[str]:
    """ê¸°ì—…ëª… ì •ë¦¬ ë° ë¶„ë¦¬"""
    # ì½¤ë§ˆ, and ë“±ìœ¼ë¡œ ë¶„ë¦¬
    companies = re.split(r'[,;]|\band\b|\bor\b', text)
    
    cleaned = []
    for company in companies:
        company = company.strip()
        # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        company = re.sub(r'\b(Inc|Corp|Corporation|LLC|Ltd|Company|Co)\b\.?', '', company, flags=re.IGNORECASE)
        company = company.strip(" .,")
        
        # ìµœì†Œ ê¸¸ì´ í™•ì¸
        if len(company) > 2 and not company.lower() in ["the", "our", "their", "such"]:
            cleaned.append(company)
    
    return cleaned


def save_document(ticker: str, content: str, sections: Dict, output_dir: Path):
    """ë¬¸ì„œ ë° ì„¹ì…˜ ì €ì¥"""
    ticker_dir = output_dir / ticker
    ticker_dir.mkdir(parents=True, exist_ok=True)
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥
    with open(ticker_dir / "full_text.txt", "w", encoding="utf-8") as f:
        f.write(content)
    
    # ì„¹ì…˜ë³„ ì €ì¥
    for section_name, section_text in sections.items():
        with open(ticker_dir / f"{section_name}.txt", "w", encoding="utf-8") as f:
            f.write(section_text)


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 70)
    print("ğŸ“¥ SEC 10-K ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ë° ê´€ê³„ ì¶”ì¶œ")
    print("=" * 70)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path("data/10k_documents")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # CIK ë§¤í•‘ ë¡œë“œ
    print("\nğŸ“‹ CIK ë§¤í•‘ ë¡œë“œ ì¤‘...")
    cik_map = get_company_cik_map()
    print(f"   {len(cik_map)}ê°œ íšŒì‚¬ ì •ë³´ ë¡œë“œë¨")
    
    headers = {
        "User-Agent": get_user_agent(),
        "Accept-Encoding": "gzip, deflate",
    }
    
    # ê´€ê³„ ë°ì´í„° ì €ì¥ìš©
    all_relationships = []
    processed_companies = []
    
    print(f"\nğŸ“Š {len(TOP_COMPANIES)}ê°œ ê¸°ì—… 10-K ì²˜ë¦¬ ì¤‘...\n")
    
    for i, ticker in enumerate(TOP_COMPANIES, 1):
        lookup_ticker = ticker.replace("-", "")
        company_info = cik_map.get(ticker) or cik_map.get(lookup_ticker)
        
        if not company_info:
            print(f"  [{i:3d}/{len(TOP_COMPANIES)}] {ticker}: âŒ CIK ì—†ìŒ")
            continue
        
        cik = company_info["cik"]
        company_name = company_info["title"]
        
        print(f"  [{i:3d}/{len(TOP_COMPANIES)}] {ticker}: {company_name[:35]:<35}", end="", flush=True)
        
        try:
            # 10-K URL ì¡°íšŒ
            doc_url, filing_date = get_10k_filing_url(cik, headers)
            
            if not doc_url:
                print(" âš ï¸ 10-K ì—†ìŒ")
                continue
            
            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
            ticker_dir = output_dir / ticker
            if (ticker_dir / "full_text.txt").exists():
                print(" â­ï¸ ì´ë¯¸ ì¡´ì¬")
                
                # ê´€ê³„ë§Œ ì¶”ì¶œ
                with open(ticker_dir / "full_text.txt", "r", encoding="utf-8") as f:
                    content = f.read()
                
                relationships = extract_relationships(content[:100000], company_name)
                all_relationships.extend(relationships)
                processed_companies.append({
                    "ticker": ticker,
                    "name": company_name,
                    "filing_date": filing_date or "unknown",
                    "relationships": len(relationships)
                })
                continue
            
            # ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ
            html_content = download_10k_document(doc_url, headers)
            
            if not html_content:
                print(" âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                continue
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_content = extract_text_from_html(html_content)
            
            # ì„¹ì…˜ ì¶”ì¶œ
            sections = extract_sections(text_content)
            
            # ì €ì¥
            save_document(ticker, text_content, sections, output_dir)
            
            # ê´€ê³„ ì¶”ì¶œ
            relationships = extract_relationships(text_content[:100000], company_name)
            all_relationships.extend(relationships)
            
            processed_companies.append({
                "ticker": ticker,
                "name": company_name,
                "filing_date": filing_date,
                "relationships": len(relationships)
            })
            
            print(f" âœ… {len(relationships)}ê°œ ê´€ê³„")
            
            # Rate limiting
            time.sleep(0.2)
        
        except Exception as e:
            print(f" âŒ ì˜¤ë¥˜: {str(e)[:30]}")
    
    # ê²°ê³¼ ì €ì¥
    print("\n" + "=" * 70)
    print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ê´€ê³„ ë°ì´í„° ì €ì¥
    relationships_df = pd.DataFrame(all_relationships)
    relationships_df.to_csv(output_dir / "relationships.csv", index=False)
    print(f"   ê´€ê³„ ë°ì´í„°: {len(all_relationships)}ê°œ â†’ relationships.csv")
    
    # JSONìœ¼ë¡œë„ ì €ì¥
    with open(output_dir / "relationships.json", "w", encoding="utf-8") as f:
        json.dump(all_relationships, f, ensure_ascii=False, indent=2)
    
    # ì²˜ë¦¬ëœ ê¸°ì—… ëª©ë¡
    processed_df = pd.DataFrame(processed_companies)
    processed_df.to_csv(output_dir / "processed_companies.csv", index=False)
    print(f"   ì²˜ë¦¬ëœ ê¸°ì—…: {len(processed_companies)}ê°œ â†’ processed_companies.csv")
    
    # ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“Š ì¶”ì¶œëœ ê´€ê³„ ìš”ì•½:")
    if not relationships_df.empty:
        rel_summary = relationships_df["type"].value_counts()
        for rel_type, count in rel_summary.items():
            print(f"   {rel_type}: {count}ê°œ")
    
    print("\nâœ… ì™„ë£Œ!")
    print(f"   ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print("=" * 70)


if __name__ == "__main__":
    main()
